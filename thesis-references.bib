@article{bader2006gtgraph,
	title={Gtgraph: A synthetic graph generator suite},
	author={Bader, David A and Madduri, Kamesh},
	journal={Atlanta, GA, February},
	year={2006}
}


@book{Myers2004AST,
	author = {Myers, Glenford J. and Sandler, Corey},
	title = {The Art of Software Testing},
	year = {2004},
	isbn = {0471469122},
	publisher = {John Wiley \& Sons},
}
@article{PointCloudPaper,
author = {Holzinger, Andreas and Malle, Bernd and Bloice, Marcus and Wiltgen, Marco and Ferri, Massimo and Stanganelli, Ignazio and Hofmann-wellenhof, Rainer},
file = {:home/bernd/Dropbox/0.topde/FWF.2014/1.PAPERS.2014/1.SOTA.2014/0.TEX/1v7\_OnPointCloudData\_20140418bm.pdf:pdf},
pages = {1--24},
title = {{On the Generation of Point Cloud Data Sets: Step One in the Knowledge Discovery Process}}
}
@article{GraphExtractPaper,
author = {Holzinger, Andreas and Malle, Bernd and Giuliani, Nicola},
doi = {10.1007/978-3-319-09891-3\_50},
file = {:home/bernd/Dropbox/0.topde/FWF.2014/1.PAPERS.2014/2.WIC.2014/1.PAPER/1v2\_GraphExtractFromImages\_FINAL\_20140525ah.pdf:pdf},
isbn = {9783319098906},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {data mining,data preprocessing,graph extraction,graph- based algorithms,graphs,image analysis,image content analytics,image segmentation,knowledge discovery},
pages = {552--563},
title = {{On graph extraction from image data}},
volume = {8609 LNAI},
year = {2014}
}
@article{Smola2011,
author = {Smola, Alex},
doi = {10.1017/CBO9781139042918},
file = {:home/bernd/develop/MScThesis/1.MY\_PAPERS/BEKKERMAN, Ron et al (2011) Scalable machine learning.pdf:pdf},
isbn = {9781139042918},
issn = {9780123814807},
title = {{Scaling Up Machine Learning}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139042918},
year = {2011}
}
@article{Kraska2013,
author = {Kraska, Tim and Talwalkar, Ameet and Duchi, John and Griffith, Rean and Franklin, Michael J and Jordan, Michael},
file = {:home/bernd/develop/MScThesis/1.MY\_PAPERS/KRASKA et al (2013) MLbase A Distributed Machine-learning System.pdf:pdf},
title = {{MLbase : A Distributed Machine-learning System}},
year = {2013}
}
@article{Low2012,
author = {Low, Yucheng and Bickson, Danny and Gonzalez, Joseph and Guestrin, Carlos and Kyrola, Aapo and Hellerstein, Joseph M.},
doi = {10.14778/2212351.2212354},
file = {:home/bernd/develop/MScThesis/1.MY\_PAPERS/LOW, Yucheng et al (2012) Distributed GraphLab - A framework for ML and DM in the Cloud.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
number = {8},
pages = {716--727},
title = {{Distributed GraphLab}},
url = {http://dl.acm.org/citation.cfm?id=2212351.2212354},
volume = {5},
year = {2012}
}
@article{Low2010,
abstract = {Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.},
archivePrefix = {arXiv},
arxivId = {1006.4990},
author = {Low, Yucheng and Gonzalez, Joseph and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos and Hellerstein, Joseph M.},
eprint = {1006.4990},
file = {:home/bernd/develop/MScThesis/1.MY\_PAPERS/LOW, Yucheng et al (2014) GraphLab - A New Framework For Parallel Machine Learning.pdf:pdf},
journal = {Conference on Uncertainty in Artificial Intelligence},
title = {{GraphLab: A New Framework for Parallel Machine Learning}},
url = {http://arxiv.org/abs/1006.4990},
year = {2010}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
eprint = {1201.0490},
file = {:home/bernd/develop/MScThesis/1.MY\_PAPERS/PEDREGOSA et al (2011) Scikit-Learn Machine Learning in Python.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {\ldots of Machine Learning \ldots},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195$\backslash$nhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Yui2013,
author = {Yui, M and Kojima, I},
file = {:home/bernd/develop/MScThesis/1.MY\_PAPERS/YUI, Makoto and KOJIMA, Isao - Hivemall Hive scalable machine learning library.pdf:pdf},
journal = {Staff.Aist.Go.Jp},
number = {2},
pages = {2013},
title = {{Hivemall: Hive scalable machine learning library}},
url = {http://staff.aist.go.jp/m.yui/publications/hivemall.pdf},
volume = {91},
year = {2013}
}
@article{optalg2014,
	author = {{Lessard}, L. and {Recht}, B. and {Packard}, A.},
	title = "{Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints}",
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1408.3595},
	primaryClass = "math.OC",
	keywords = {Mathematics - Optimization and Control, Computer Science - Numerical Analysis, Computer Science - Systems and Control},
	year = 2014,
	month = aug,
	adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1408.3595L},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{velox2015,
	author    = {Daniel Crankshaw and	Peter Bailis and	Joseph E. Gonzalez and	Haoyuan Li and	Zhao Zhang and	Michael J. Franklin and	Ali Ghodsi and	Michael I. Jordan},
	title     = {The Missing Piece in Complex Analytics: Low Latency, Scalable Model	Management and Serving with Velox},
	journal   = {CoRR},
	volume    = {abs/1409.3809},
	year      = {2014},
	url       = {http://arxiv.org/abs/1409.3809},
	timestamp = {Wed, 01 Oct 2014 15:00:05 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/CrankshawBGLZFGJ14},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{FelzenszwalbHuttenlocher2004,
	year = {2004},
	doi = {10.1023/B:VISI.0000022288.19776.77},
	author = {Felzenszwalb, Pedro F and Huttenlocher, Daniel P},
	title = {Efficient graph-based image segmentation},
	journal = {International Journal of Computer Vision},
	volume = {59},
	number = {2},
	pages = {167-181},
	abstract = {This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.}
} 

@article{LeeObjectGraphs2012,
abstract = {How can knowing about some categories help us to discover new ones in unlabeled images? Unsupervised visual category discovery is useful to mine for recurring objects without human supervision, but existing methods assume no prior information and thus tend to perform poorly for cluttered scenes with multiple objects. We propose to leverage knowledge about previously learned categories to enable more accurate discovery, and address challenges in estimating their familiarity in unsegmented, unlabeled images. We introduce two variants of a novel object-graph descriptor to encode the 2D and 3D spatial layout of object-level co-occurrence patterns relative to an unfamiliar region and show that by using them to model the interaction between an imageâ€™s known and unknown objects, we can better detect new visual categories. Rather than mine for all categories from scratch, our method identifies new objects while drawing on useful cues from familiar ones. We evaluate our approach on several benchmark data sets and demonstrate clear improvements in discovery over conventional purely appearance-based baselines.},
author = {Lee, Yong Jae and Grauman, Kristen},
doi = {10.1109/TPAMI.2011.122},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = feb,
number = {2},
pages = {346--58},
pmid = {21670480},
title = {{Object-graphs for context-aware visual category discovery.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21670480},
volume = {34},
year = {2012}
}
@article{GraphCutSegmentation2013,
author = {Wang, Xiaofang and Li, Huibin and Bichot, Charles-Edmond and Masnou, Simon and Chen, Liming},
doi = {10.1109/ICIP.2013.6738828},
file = {:home/bernd/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2013 - A graph-cut approach to image segmentation using an affinity graph based on {$\ell$}inf0inf-sparse representation of fea.pdf:pdf},
isbn = {978-1-4799-2341-0},
journal = {2013 IEEE International Conference on Image Processing},
keywords = {image segmentation,sparse representation},
month = sep,
pages = {4019--4023},
publisher = {Ieee},
title = {{A graph-cut approach to image segmentation using an affinity graph based on {$\ell$} <inf>0</inf>-sparse representation of features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6738828},
year = {2013}
}
@article{BeliefPropBillionNodes2010,
author = {Kang, U and Horng, Duen},
file = {:home/bernd/develop/MScThesis/1.MY\_PAPERS/KANG, CHAU, FALOUTSOS (2010) Belief\_Propagation\_Billion\_Node\_Graphs.pdf:pdf},
isbn = {9781450302159},
keywords = {all or part of,belief propagation,gim-v,hadoop,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for},
title = {{Inference of beliefs on billion-scale graphs}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.188.5276},
year = {2010}
}
@article{BeliefPropFraudDetection2007,
abstract = {Given a large online network of online auction users and their histories of transactions, how can we spot anomalies and auction fraud? This paper describes the design and implementation of NetProbe, a system that we propose for solving this problem. NetProbe models auction users and transactions as a Markov Random Field tuned to detect the suspicious patterns that fraudsters create, and employs a Belief Propagation mechanism to detect likely fraudsters. Our experiments show that NetProbe is both efficient and effective for fraud detection. We report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges, where NetProbe was able to spot fraudulent nodes with over 90\% precision and recall, within a matter of seconds. We also report experiments on a real dataset crawled from eBay, with nearly 700,000 transactions between more than 66,000users, where NetProbe was highly effective at unearthing hidden networks of fraudsters, within a realistic response time of about 6 minutes. For scenarios where the underlying data is dynamic in nature, we propose IncrementalNetProbe, which is an approximate, but fast, variant of NetProbe. Our experiments prove that Incremental NetProbe executes nearly doubly fast as compared to NetProbe, while retaining over 99\% of its accuracy.},
author = {Pandit, Shashank and Chau, DH and Wang, Samuel and Faloutsos, Christos},
doi = {10.1145/1242572.1242600},
file = {:home/bernd/develop/MScThesis/1.MY\_PAPERS/PANDIT et al (2007) Net\_Probe\_Fraud\_Detection\_Online\_Auction\_Networks.pdf:pdf},
isbn = {9781595936547},
journal = {Proceedings of the 16th \ldots},
pages = {210, 201},
title = {{Netprobe: a fast and scalable system for fraud detection in online auction networks}},
url = {http://dx.doi.org/10.1145/1242572.1242600$\backslash$nhttp://dl.acm.org/citation.cfm?id=1242600},
volume = {42},
year = {2007}
}
@article{Buyya2005,
author = {Buyya, Rajkumar and Venugopal, Srikumar},
file = {:home/bernd/Desktop/GridIntro-CSI2005-gentle.pdf:pdf},
journal = {Computer Society of India},
keywords = {and globus,e-science,grid computing,grid middleware,gridbus},
number = {July},
pages = {9--19},
title = {{A Gentle Introduction to Grid Computing and Technologies}},
year = {2005}
}
@article{Joshi2005,
author = {Joshi, Mahesh},
file = {:home/bernd/Desktop/grid-computing.pdf:pdf},
number = {April},
pages = {1--17},
title = {{Grid Computing}},
year = {2005}
}
@article{Foster2008,
abstract = {Cloud Computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for Cloud Computing and there seems to be no consensus on what a Cloud is. On the other hand, Cloud Computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established Grid Computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast Cloud Computing with Grid Computing from various angles and give insights into the essential characteristics of both.},
archivePrefix = {arXiv},
arxivId = {0901.0131},
author = {Foster, Ian and Zhao, Yong and Raicu, Ioan and Lu, Shiyong},
doi = {10.1109/GCE.2008.4738445},
eprint = {0901.0131},
file = {:home/bernd/Desktop/cloud\_computing\_and\_grid\_computing.pdf:pdf},
isbn = {978-1-4244-2860-1},
journal = {2008 Grid Computing Environments Workshop},
pages = {1--10},
title = {{Cloud Computing and Grid Computing 360-Degree Compared}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4738445},
year = {2008}
}
@article{Abramson2002,
abstract = {Computational grids that couple geographically distributed resources such as PCs, workstations, clusters, and scientific instruments, have emerged as a next generation computing platform for solving large-scale problems in science, engineering, and commerce. However, application development, resource management, and scheduling in these environments continue to be a complex undertaking. In this article, we discuss our efforts in developing a resource management system for scheduling computations on resources distributed across the world with varying quality of service (QoS). Our service-oriented grid computing system called Nimrod-G manages all operations associated with remote execution including resource discovery, trading, scheduling based on economic principles and a user-defined QoS requirement. The Nimrod-G resource broker is implemented by leveraging existing technologies such as Globus, and provides new services that are essential for constructing industrial-strength grids. We present the results of experiments using the Nimrod-G resource broker for scheduling parametric computations on the World Wide Grid (WWG) resources that span five continents. ?? 2002 Elsevier Science B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {cs/0111048},
author = {Abramson, David and Buyya, Rajkumar and Giddy, Jonathan},
doi = {10.1016/S0167-739X(02)00085-7},
eprint = {0111048},
file = {:home/bernd/Desktop/A computational economy for grid computing.pdf:pdf},
isbn = {0167-739X},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Computational economy,Grid computing,Grid scheduling,Nimrod-G broker,Resource management},
number = {8},
pages = {1061--1074},
primaryClass = {cs},
title = {{A computational economy for grid computing and its implementation in the Nimrod-G resource broker}},
volume = {18},
year = {2002}
}
@article{Pearce2007,
abstract = {We consider how to maintain the topological order of a directed acyclic graph (DAG) in the presence of edge insertions and deletions. We present a new algorithm and, although this has marginally inferior time complexity compared with the best previously known result, we find that its simplicity leads to better performance in practice. In addition, we provide an empirical comparison against three alternatives over a large number of random DAGâ€™s. The results show our algorithm is the best for sparse graphs and, surprisingly, that an alternative with poor theoretical complexity performs marginally better on dense graphs},
author = {Pearce, David J. and Kelly, Paul H. J.},
doi = {10.1145/1187436.1210590},
file = {:home/bernd/Desktop/DynamicTopoSortAlg-JEA-07.pdf:pdf},
issn = {10846654},
journal = {Journal of Experimental Algorithmics},
number = {1},
pages = {1.7},
title = {{A dynamic topological sort algorithm for directed acyclic graphs}},
volume = {11},
year = {2007}
}







@ONLINE{LargeScaleMLPipelines,
  title = {Building and deploying large-scale machine learning pipelines},
  url = {http://radar.oreilly.com/2015/01/building-and-deploying-large-scale-machine-learning-pipelines.html},
  author = {Lorica, Ben},
  year = {2015},
  organization = {Oâ€™Reilly Media, Inc.}
}
@ONLINE{MLPipelineMLlib,
  title = {ML Pipelines: A New High-Level API for MLlib},
  url = {https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html},
  author = {Meng, Xiangrui},
  year = {2015},
  organization = {Databricks Inc.}
}
@ONLINE{DataAnalysisDSWorkflow,
  title = {Data Analysis: Just one component of the Data Science workflow},
  url = {http://radar.oreilly.com/2013/09/data-analysis-just-one-component-of-the-data-science-workflow.html},
  author = {Lorica, Ben},
  year = {2013},
  organization = {Oâ€™Reilly Media, Inc.}
}
@ONLINE{StanfordNLP,
  title = {The Stanford Natural Language Processing Group},
  url = {http://nlp.stanford.edu/},
  author = {Stanford NLP Group},
  year = {},
  organization = {Stanford University}
}
@ONLINE{MLPipelines,
  title = {ML Pipelines},
  url = {https://amplab.cs.berkeley.edu/ml-pipelines/},
  author = {Sparks, Evan},
  year = {2014},
  organization = {UC Berkeley}
}
@ONLINE{Make2013,
  title = {GNU Make for Reproducible Data Analysis},
  url = {http://zmjones.com/make/},
  author = {Zachary M. Jones},
  year = {2013},
  organization = {Pennsylvania State University}
}
@ONLINE{AnalLifecycle,
  title = {Data scientists tackle the analytic lifecycle},
  url = {http://radar.oreilly.com/2013/07/data-scientists-and-the-analytic-lifecycle.html},
  author = {Lorica, Ben},
  year = {2013},
  organization = {Oâ€™Reilly Media, Inc.}
}
@ONLINE{DataScienceTools2013,
  title = {Data Science Tools: Fast, easy to use, and scalable},
  url = {http://radar.oreilly.com/2013/03/fast-easy-to-use-scalable-data-science-tools.html},
  author = {Lorica, Ben},
  year = {2013},
  organization = {Oâ€™Reilly Media, Inc.}
}
@ONLINE{AnalOneComponent2013,
  title = {Data Analysis: Just one component of the Data Science workflow},
  url = {http://radar.oreilly.com/2013/09/data-analysis-just-one-component-of-the-data-science-workflow.html},
  author = {Lorica, Ben},
  year = {2013},
  organization = {Oâ€™Reilly Media, Inc.}
}
@ONLINE{JSDocRef,
  title = {Choosing a JavaScript Documentation Generator â€“ JSDoc vs YUIDoc vs Doxx vs Docco},
  url = {http://www.fusioncharts.com/blog/2013/12/jsdoc-vs-yuidoc-vs-doxx-vs-docco-choosing-a-javascript-documentation-generator/},
  author = {Das Modak, Kaustav},
  year = {2016},
  organization = {FusionBrew}
}
@ONLINE{CucumberJS,
	title = {Cucumber for JavaScript},
	url = {https://github.com/cucumber/cucumber-js},
	author = {Biezemans, Julien},
	year = {2016},
	organization = {cucumber.io}
}
@ONLINE{JSTestTest,
	title = {Jasmine vs. Mocha, Chai, and Sinon},
	url = {http://thejsguy.com/2015/01/12/jasmine-vs-mocha-chai-and-sinon.html},
	author = {Tang, David},
	year = {2015},
	organization = {THE JS GUY}
}
@ONLINE{LessCSS,
	title = {An overview of Less, how to download and use, examples and more.},
	url = {http://lesscss.org/},
	author = {Page, Lukas and Mikhailov, Max},
	year = {2016},
	organization = {lesscss.org}
}
@ONLINE{SassBasics,
	title = {Sass Basics},
	url = {http://sass-lang.com/guide},
	author = {Hampton, Catlin and Natalie, Weizenbaum and Chris Eppstein.},
	year = {2016},
	organization = {sass-lang.com}
}
@ONLINE{CoffeeScript,
	title = {10 CoffeeScript One Liners to Impress Your Friends},
	url = {http://ricardo.cc/2011/06/02/10-CoffeeScript-One-Liners-to-Impress-Your-Friends.html},
	author = {Tomasi, Ricardo},
	year = {2011},
	organization = {ricardo.cc}
}
@ONLINE{ES6Features,
	title = {ECMAScript 6 - New Features: Overview \& Comparison},
	url = {http://es6-features.org/},
	author = {Engelschall, Ralf S.},
	year = {2016},
	organization = {es6-features.org}
}



%OGMAmed References LAST UPDATED 05.09.2015 12:00

@article{Abramson2002,
	abstract = {Computational grids that couple geographically distributed resources such as PCs, workstations, clusters, and scientific instruments, have emerged as a next generation computing platform for solving large-scale problems in science, engineering, and commerce. However, application development, resource management, and scheduling in these environments continue to be a complex undertaking. In this article, we discuss our efforts in developing a resource management system for scheduling computations on resources distributed across the world with varying quality of service (QoS). Our service-oriented grid computing system called Nimrod-G manages all operations associated with remote execution including resource discovery, trading, scheduling based on economic principles and a user-defined QoS requirement. The Nimrod-G resource broker is implemented by leveraging existing technologies such as Globus, and provides new services that are essential for constructing industrial-strength grids. We present the results of experiments using the Nimrod-G resource broker for scheduling parametric computations on the World Wide Grid (WWG) resources that span five continents.},
	archivePrefix = {arXiv},
	arxivId = {cs/0111048},
	author = {Abramson, David and Buyya, Rajkumar and Giddy, Jonathan},
	doi = {10.1016/S0167-739X(02)00085-7},
	eprint = {0111048},
	file = {:home/bernd/Desktop/A computational economy for grid computing.pdf:pdf},
	isbn = {0167-739X},
	issn = {0167739X},
	journal = {Future Generation Computer Systems},
	keywords = {Computational economy,Grid computing,Grid scheduling,Nimrod-G broker,Resource management},
	number = {8},
	pages = {1061--1074},
	primaryClass = {cs},
	title = {{A computational economy for grid computing and its implementation in the Nimrod-G resource broker}},
	volume = {18},
	year = {2002}
}

@article{Armbrust2010,
	abstract = {Clearing the clouds away from the true potential and obstacles posed by this computing capability.},
	archivePrefix = {arXiv},
	arxivId = {0521865719 9780521865715},
	author = {Armbrust, Michael and Stoica, Ion and Zaharia, Matei and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel},
	doi = {10.1145/1721654.1721672},
	eprint = {0521865719 9780521865715},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/p50-armbrust.pdf:pdf},
	isbn = {9781605589336},
	issn = {00010782},
	journal = {Communications of the ACM},
	number = {4},
	pages = {50},
	pmid = {11242594},
	title = {{A view of cloud computing}},
	volume = {53},
	year = {2010}
}

@book{Burke:2003:Hyperheuristics,
	year = {2003},
	author = {Burke, Edmund and Kendall, Graham and Newall, Jim and Hart, Emma and Ross, Peter and Schulenburg, Sonia},
	title = {Hyper-Heuristics: An Emerging Direction in Modern Search Technology},
	booktitle = {Handbook of Metaheuristics, Volume 57 International Series in Operations Research and Management Science},
	editor = {Glover, Fred and Kochenberger, GaryA},
	publisher = {Springer},
	address = {New York},
	pages = {457-474},
	abstract = {This chapter introduces and overviews an emerging methodology in search and optimisation. One of the key aims of these new approaches, which have been termed hyperheuristics, is to raise the level of generality at which optimisation systems can operate. An objective is that hyper-heuristics will lead to more general systems that are able to handle a wide range of problem domains rather than current meta-heuristic technology which tends to be customised to a particular problem or a narrow class of problems. Hyper-heuristics are broadly concerned with intelligently choosing the right heuristic or algorithm in a given situation. Of course, a hyper-heuristic can be (often is) a (meta-)heuristic and it can operate on (meta-)heuristics. In a certain sense, a hyper-heuristic works at a higher level when compared with the typical application of meta-heuristics to optimisation problems, i.e., a hyper-heuristic could be thought of as a (meta)-heuristic which operates on lower level (meta-)heuristics. In this chapter we will introduce the idea and give a brief history of this emerging area. In addition, we will review some of the latest work to be published in the field.},
	keywords = {Hyper-heuristic, Meta-heuristic, Optimisation, Search},
	doi = {10.1007/0-306-48056-5_16},
	url = {http://dx.doi.org/10.1007/0-306-48056-5_16}
}

@book{Holzinger:2014:SpringerTextbook,
	year = {2014},
	author = {Holzinger, Andreas},
	title = {Biomedical Informatics: Discovering Knowledge in Big Data},
	publisher = {Springer},
	address = {New York},
	abstract = {This book provides a broad overview of the topic Bioinformatics (medical informatics + biological information) with a focus on data, information and knowledge. From data acquisition and storage to visualization, privacy, regulatory, and other practical and theoretical topics, the author touches on several fundamental aspects of the innovative interface between the medical and computational domains that form biomedical informatics. Each chapter starts by providing a useful inventory of definitions and commonly used acronyms for each topic, and throughout the text, the reader finds several real-world examples, methodologies, and ideas that complement the technical and theoretical background. Also at the beginning of each chapter a new section called key problems, has been added, where the author discusses possible traps and unsolvable or major problems. This new edition includes new sections at the end of each chapter, called future outlook and research avenues, providing pointers to future challenges.},
	doi = {10.1007/978-3-319-04528-3}
}

@book{Holzinger:2011:InformationQuality,
	year = {2011},
	author = {Holzinger, Andreas and Simonic, Klaus-Martin},
	title = {Information Quality in e-Health. Lecture Notes in Computer Science LNCS 7058},
	publisher = {Springer},
	address = {Heidelberg, Berlin, New York},
	abstract = {Medical information systems are already highly sophisticated; however, while computer performance has increased exponentially, human cognitive evolution cannot advance at the same speed. Consequently, the focus on interaction and communication between humans and computers is of increasing importance in medicine and healthcare. The daily actions of medical professionals must be the central concern of any innovation. Simply surrounding and supporting them with new and emerging technologies is not sufficient if these increase rather than decrease the workload. Information systems are a central component of modern knowledge-based medicine and health services; therefore, it is necessary for knowledge management to continually be adapted to the needs and demands of medical professionals within this environment of steadily increasing high-tech medicine. Information processing, in particular its potential effectiveness in modern health services and the optimization ofprocesses and operational sequences, is also of increasing interest. It is particularly important for medical information systems (e.g., hospital information systems and decision support systems) to be designed with the daily schedules, responsibilities and exigencies of the medical professionals in mind. Within the context of this symposium our end users are medical professionals and justifiably expect the software technology to provide a clear benefit: to support them efficiently and effectively in their daily activities. In biomedicine, healthcare, clinical medicine and the life sciences, professional end users are confronted with an increased mass of data. Research in human-computer interaction (HCI) and information retrieval (IR) or knowledge discovery in databases and data mining (KDD) has long been working to develop methods that help users to identify, extract, visualize and understand useful information from these masses of high-dimensional and mostly weakly structured data. HCI and IR/KDD, however, take very different perspectives in tackling this challenge; and historically, they have had little collaboration. Our goal is to combine these efforts to support professionals in interactively analyzing information properties and visualizing the relevant information without becoming overwhelmed. The challenge is to bring HCI and IR/KDD researchers to work together and hence reap the benefits that computer science/informatics can provide to the areas of medicine, healthcare and the life sciences },
	keywords = {Information Quality, Knowledge},
	doi = {10.1007/978-3-642-25364-5}
}


@article{HolzingerEtAl:2014:ResearchChallenges,
	year = {2014},
	author = {Holzinger, Andreas and Dehmer, Matthias and Jurisica, Igor},
	title = {Knowledge Discovery and interactive Data Mining in Bioinformatics - State-of-the-Art, future challenges and research directions},
	journal = {BMC Bioinformatics},
	volume = {15},
	number = {S6},
	pages = {I1},
	abstract = {The life sciences, biomedicine and health care are increasingly turning into a data intensive science. Particularly in bioinformatics and computational biology we face not only increased volume and a diversity of highly complex, multi-dimensional and often weakly-structured and noisy data, but also the growing need for integrative analysis and modeling. Due to the increasing trend towards personalized and precision medicine (P4 medicine: Predictive, Preventive, Participatory, Personalized), biomedical data today results from various sources in different structural dimensions, ranging from the microscopic world, and in particular from the omics world (e.g., from genomics, proteomics, metabolomics, lipidomics, transcriptomics, epigenetics, microbiomics, fluxomics, phenomics, etc.) to the macroscopic world (e.g., disease spreading data of populations in public health informatics). The challenge is not only to extract meaningful information from this data, but to gain knowledge, to discover previously unknown insight, look for patterns, and to make sense of the data. },
	keywords = {Knowledge Discovery, Interactive Data Mining, Bioinformatics, Biomedical Informatics, Data intensive Science},
	doi = {doi:10.1186/1471-2105-15-S6-I1},
	url = {http://www.biomedcentral.com/1471-2105/15/S6/I1}
}

@inproceedings{Halevy:2012:Ecosystem,
	author = {Halevy, Alon Y.},
	title = {Towards an Ecosystem of Structured Data on the Web},
	booktitle = {Proceedings of the 15th International Conference on Extending Database Technology},
	series = {EDBT '12},
	year = {2012},
	isbn = {978-1-4503-0790-1},
	location = {Berlin, Germany},
	pages = {1--2},
	numpages = {2},
	url = {http://doi.acm.org/10.1145/2247596.2247597},
	doi = {10.1145/2247596.2247597},
	acmid = {2247597},
	publisher = {ACM},
	address = {New York, NY, USA}
}

@inproceedings{GuptaEtAl2013ProgressonHalevy,
	year = {2013},
	author = {Gupta, N. and Halevy, A. Y. and Harb, B. and Lam, H. and Hongrae, Lee and Madhavan, J. and Fei, Wu and Cong, Yu},
	title = {Recent progress towards an ecosystem of structured data on the Web},
	booktitle = {29th International IEEE Conference on Data Engineering (ICDE)},
	pages = {5-8},
	abstract = {Google Fusion Tables aims to support an ecosystem of structured data on the Web by providing a tool for managing and visualizing data on the one hand, and for searching and exploring for data on the other. This paper describes a few recent developments in our efforts to further the ecosystem.},
	keywords = {Google fusion tables, ecosystem},
	doi = {10.1109/ICDE.2013.6544808},
	url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6544808}
}


@article{Pearce2007,
	abstract = {We consider how to maintain the topological order of a directed acyclic graph (DAG) in the presence of edge insertions and deletions. We present a new algorithm and, although this has marginally inferior time complexity compared with the best previously known result, we find that its simplicity leads to better performance in practice. In addition, we provide an empirical comparison against three alternatives over a large number of random DAGs. The results show our algorithm is the best for sparse graphs and, surprisingly, that an alternative with poor theoretical complexity performs marginally better on dense graphs},
	author = {Pearce, David J. and Kelly, Paul H. J.},
	doi = {10.1145/1187436.1210590},
	file = {:home/bernd/Desktop/DynamicTopoSortAlg-JEA-07.pdf:pdf},
	issn = {10846654},
	journal = {Journal of Experimental Algorithmics},
	number = {1},
	pages = {1.7},
	title = {{A dynamic topological sort algorithm for directed acyclic graphs}},
	volume = {11},
	year = {2007}
}

@article{Buyya2005,
	author = {Buyya, Rajkumar and Venugopal, Srikumar},
	file = {:home/bernd/Desktop/GridIntro-CSI2005-gentle.pdf:pdf},
	journal = {Computer Society of India},
	keywords = {and globus,e-science,grid computing,grid middleware,gridbus},
	number = {July},
	pages = {9--19},
	title = {{A Gentle Introduction to Grid Computing and Technologies}},
	year = {2005}
}

@article{Foster2008,
	abstract = {Cloud Computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for Cloud Computing and there seems to be no consensus on what a Cloud is. On the other hand, Cloud Computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established Grid Computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast Cloud Computing with Grid Computing from various angles and give insights into the essential characteristics of both.},
	archivePrefix = {arXiv},
	arxivId = {0901.0131},
	author = {Foster, Ian and Zhao, Yong and Raicu, Ioan and Lu, Shiyong},
	doi = {10.1109/GCE.2008.4738445},
	eprint = {0901.0131},
	file = {:home/bernd/Desktop/cloud\_computing\_and\_grid\_computing.pdf:pdf},
	isbn = {978-1-4244-2860-1},
	journal = {2008 Grid Computing Environments Workshop},
	pages = {1--10},
	title = {{Cloud Computing and Grid Computing 360-Degree Compared}},
	year = {2008}
}

@article{Joshi2005,
	author = {Joshi, Mahesh},
	file = {:home/bernd/Desktop/grid-computing.pdf:pdf},
	number = {April},
	pages = {1--17},
	title = {{Grid Computing}},
	year = {2005}
}

@article{Kang2010,
	author = {Kang, U and Horng, Duen},
	file = {::},
	isbn = {9781450302159},
	keywords = {all or part of,belief propagation,gim-v,hadoop,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for},
	title = {{Inference of beliefs on billion-scale graphs}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.188.5276},
	year = {2010}
}
@article{Pandit2007,
	abstract = {Given a large online network of online auction users and their histories of transactions, how can we spot anomalies and auction fraud? This paper describes the design and implementation of NetProbe, a system that we propose for solving this problem. NetProbe models auction users and transactions as a Markov Random Field tuned to detect the suspicious patterns that fraudsters create, and employs a Belief Propagation mechanism to detect likely fraudsters. Our experiments show that NetProbe is both efficient and effective for fraud detection. We report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges, where NetProbe was able to spot fraudulent nodes with over 90\% precision and recall, within a matter of seconds. We also report experiments on a real dataset crawled from eBay, with nearly 700,000 transactions between more than 66,000users, where NetProbe was highly effective at unearthing hidden networks of fraudsters, within a realistic response time of about 6 minutes. For scenarios where the underlying data is dynamic in nature, we propose IncrementalNetProbe, which is an approximate, but fast, variant of NetProbe. Our experiments prove that Incremental NetProbe executes nearly doubly fast as compared to NetProbe, while retaining over 99\% of its accuracy.},
	author = {Pandit, Shashank and Chau, DH and Wang, Samuel and Faloutsos, Christos},
	doi = {10.1145/1242572.1242600},
	file = {::},
	isbn = {9781595936547},
	journal = {Proceedings of the 16th \ldots},
	pages = {210, 201},
	title = {{Netprobe: a fast and scalable system for fraud detection in online auction networks}},
	url = {http://dx.doi.org/10.1145/1242572.1242600$\backslash$nhttp://dl.acm.org/citation.cfm?id=1242600},
	volume = {42},
	year = {2007}
}
@article{Felzenszwalb2004,
	author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
	doi = {10.1023/B:VISI.0000022288.19776.77},
	file = {::},
	issn = {0920-5691},
	journal = {International Journal of Computer Vision},
	keywords = {clustering,graph algorithm,image segmentation,perceptual organization},
	month = sep,
	number = {2},
	pages = {167--181},
	title = {{Efficient Graph-Based Image Segmentation}},
	url = {http://link.springer.com/10.1023/B:VISI.0000022288.19776.77},
	volume = {59},
	year = {2004}
}
@article{Lee2012,
	abstract = {How can knowing about some categories help us to discover new ones in unlabeled images? Unsupervised visual category discovery is useful to mine for recurring objects without human supervision, but existing methods assume no prior information and thus tend to perform poorly for cluttered scenes with multiple objects. We propose to leverage knowledge about previously learned categories to enable more accurate discovery, and address challenges in estimating their familiarity in unsegmented, unlabeled images. We introduce two variants of a novel object-graph descriptor to encode the 2D and 3D spatial layout of object-level co-occurrence patterns relative to an unfamiliar region and show that by using them to model the interaction between an imageâ€™s known and unknown objects, we can better detect new visual categories. Rather than mine for all categories from scratch, our method identifies new objects while drawing on useful cues from familiar ones. We evaluate our approach on several benchmark data sets and demonstrate clear improvements in discovery over conventional purely appearance-based baselines.},
	author = {Lee, Yong Jae and Grauman, Kristen},
	doi = {10.1109/TPAMI.2011.122},
	issn = {1939-3539},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	month = feb,
	number = {2},
	pages = {346--58},
	pmid = {21670480},
	title = {{Object-graphs for context-aware visual category discovery.}},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21670480},
	volume = {34},
	year = {2012}
}
@article{Wang2013,
	author = {Wang, Xiaofang and Li, Huibin and Bichot, Charles-Edmond and Masnou, Simon and Chen, Liming},
	doi = {10.1109/ICIP.2013.6738828},
	file = {::},
	isbn = {978-1-4799-2341-0},
	journal = {2013 IEEE International Conference on Image Processing},
	keywords = {image segmentation,sparse representation},
	month = sep,
	pages = {4019--4023},
	publisher = {Ieee},
	title = {{A graph-cut approach to image segmentation using an affinity graph based on â„“<inf>0</inf>-sparse representation of features}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6738828},
	year = {2013}
}
@article{Kraska2013,
	author = {Kraska, Tim and Talwalkar, Ameet and Duchi, John and Griffith, Rean and Franklin, Michael J and Jordan, Michael},
	file = {::},
	title = {{MLbase : A Distributed Machine-learning System}},
	year = {2013}
}
@article{Low2010,
	abstract = {Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.},
	archivePrefix = {arXiv},
	arxivId = {1006.4990},
	author = {Low, Yucheng and Gonzalez, Joseph and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos and Hellerstein, Joseph M.},
	eprint = {1006.4990},
	file = {:home/bernd/develop/MScThesis/1.MY\_PAPERS/LOW, Yucheng et al (2014) GraphLab - A New Framework For Parallel Machine Learning.pdf:pdf},
	journal = {Conference on Uncertainty in Artificial Intelligence},
	title = {{GraphLab: A New Framework for Parallel Machine Learning}},
	url = {http://arxiv.org/abs/1006.4990},
	year = {2010}
}
@article{Pedregosa2012,
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	archivePrefix = {arXiv},
	arxivId = {1201.0490},
	author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
	eprint = {1201.0490},
	file = {::},
	isbn = {1532-4435},
	issn = {15324435},
	journal = {\ldots of Machine Learning \ldots},
	pages = {2825--2830},
	title = {{Scikit-learn: Machine Learning in Python}},
	url = {http://dl.acm.org/citation.cfm?id=2078195$\backslash$nhttp://arxiv.org/abs/1201.0490},
	volume = {12},
	year = {2012}
}
@article{Smola2011,
	author = {Smola, Alex},
	doi = {10.1017/CBO9781139042918},
	file = {::},
	isbn = {9781139042918},
	issn = {9780123814807},
	title = {{Scaling Up Machine Learning}},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139042918},
	year = {2011}
}
@article{Yui2013,
	author = {Yui, M and Kojima, I},
	file = {::},
	journal = {Staff.Aist.Go.Jp},
	number = {2},
	pages = {2013},
	title = {{Hivemall: Hive scalable machine learning library}},
	url = {http://staff.aist.go.jp/m.yui/publications/hivemall.pdf},
	volume = {91},
	year = {2013}
}
@article{Crankshaw2015,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1409.3809v2},
	author = {Crankshaw, Daniel and Bailis, Peter and Gonzalez, Joseph E and Li, Haoyuan and Zhang, Zhao and Franklin, Michael J and Ghodsi, Ali and Jordan, Michael I and Amplab, U C Berkeley},
	eprint = {arXiv:1409.3809v2},
	file = {::},
	journal = {CIDR (to appear)},
	title = {{The Missing Piece in Complex Analytics : Low Latency , Scalable Model Management and Serving with Velox}},
	year = {2015}
}
@article{Lessard2014,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1408.3595v3},
	author = {Lessard, Laurent and Dec, O C},
	eprint = {arXiv:1408.3595v3},
	file = {::},
	keywords = {control theory,convex optimization,first-order methods,heavy-ball,integral quadratic con-,method,nesterov,proximal gradient methods,s method,semidefinite programming,straints},
	pages = {1--40},
	title = {{Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints}},
	year = {2014}
}
@article{Low2012,
	abstract = {While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.},
	archivePrefix = {arXiv},
	arxivId = {1204.6078},
	author = {Low, Yucheng and Bickson, Danny and Gonzalez, Joseph and Guestrin, Carlos and Kyrola, Aapo and Hellerstein, Joseph M},
	doi = {10.14778/2212351.2212354},
	eprint = {1204.6078},
	file = {::},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	keywords = {a framework for machine,and data mining in,learning,the cloud,tributed graphlab},
	number = {8},
	pages = {716--727},
	title = {{Distributed GraphLab: a framework for machine learning and data mining in the cloud}},
	url = {http://dl.acm.org/citation.cfm?id=2212354},
	volume = {5},
	year = {2012}
}
@article{Figueiredo2003,
	abstract = { We advocate a novel approach to grid computing that is based on a combination of "classic" operating system level virtual machines (VMs) and middleware mechanisms to manage VMs in a distributed environment. The abstraction is that of dynamically instantiated and mobile VMs that are a combination of traditional OS processes (the VM monitors) and files (the VM state). We give qualitative arguments that justify our approach in terms of security, isolation, customization, legacy support and resource control, and we show quantitative results that demonstrate the feasibility of our approach front a performance perspective. Finally, we describe the middleware challenges implied by the approach and an architecture for grid computing using virtual machines.},
	author = {Figueiredo, R.J. and Dinda, P.a. and Fortes, J.a.B.},
	doi = {10.1109/ICDCS.2003.1203506},
	file = {::},
	isbn = {0-7695-1920-2},
	issn = {1063-6927},
	journal = {23rd International Conference on Distributed Computing Systems, 2003. Proceedings.},
	title = {{A case for grid computing on virtual machines}},
	year = {2003}
}
@article{Rodriguez2009,
	abstract = {Virtual machines can greatly simplify grid computing by providing an isolated, well-known environment, while increasing security. Also, they can be used as the base technology to dynamically modify the computing elements of a grid, so providing an adaptive environment. In this paper we present a Grid architecture that allows to dynamically adapt the underlying hardware infrastructure to changing Virtual Organization (VO) demands. The backend of the system is able to provide on-demand virtual worker nodes to existing clusters and integrate them in any Globus-based Grid. In this way, we establish a basis to deploy self-adaptive Grids, which can support different VOs in shared physical infrastructures and dynamically adapt its software configuration. Experimental results on a prototyped testbed show less than a 10\% overall performance loss including the hypervisor overhead.},
	author = {Rodr\'{\i}guez, Manuel and Tapiador, Daniel and Font\'{a}n, Javier and Huedo, Eduardo and Montero, Rub\'{e}n S. and Llorente, Ignacio M.},
	doi = {10.1007/978-3-642-00955-6\_4},
	file = {::},
	isbn = {3642009549},
	issn = {03029743},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in)},
	number = {215605},
	pages = {23--32},
	title = {{Dynamic provisioning of virtual clusters for grid computing}},
	volume = {5415 LNCS},
	year = {2009}
}
@article{Sotomayor2007,
	author = {Sotomayor, Borja},
	year = {2007},
	file = {::},
	title = {{Virtual Machines for Grid Computing}}
}


@inproceedings{MLTechnicalDebt,
	title = {Machine Learning: The High Interest Credit Card of Technical Debt},
	author  = {D. Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young},
	year  = 2014,
	booktitle = {SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)}
}
@article{Tordsson2012,
	abstract = {In the past few years, we have witnessed the proliferation of a heterogeneous ecosystem of cloud providers, each one with a different infrastructure offer and pricing policy. We explore this heterogeneity in a novel cloud brokering approach that optimizes placement of virtual infrastructures across multiple clouds and also abstracts the deployment and management of infrastructure components in these clouds. The feasibility of our approach is evaluated in a high throughput computing cluster case study. Experimental results confirm that multi-cloud deployment provides better performance and lower costs compared to the usage of a single cloud only. Â© 2011 Elsevier B.V. All rights reserved.},
	author = {Tordsson, Johan and Montero, Rub\'{e}n S. and Moreno-Vozmediano, Rafael and Llorente, Ignacio M.},
	doi = {10.1016/j.future.2011.07.003},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/TORDSSON (2012) Cloud brokering mechanisms for optimized placement of virtual machines across multiple providers.pdf:pdf},
	isbn = {0167-739X},
	issn = {0167739X},
	journal = {Future Generation Computer Systems},
	keywords = {Cloud computing,Infrastructure as a Service (IaaS),Interoperability,Scheduling},
	number = {2},
	pages = {358--367},
	publisher = {Elsevier B.V.},
	title = {{Cloud brokering mechanisms for optimized placement of virtual machines across multiple providers}},
	url = {http://dx.doi.org/10.1016/j.future.2011.07.003},
	volume = {28},
	year = {2012}
}
@article{Huang2006,
	abstract = {Virtual machine (VM) technologies are experiencing a resurgence in both industry and research communities. VMs offer many desirable features such as security, ease of management, OS customization, performance isolation, check-pointing, and migration, which can be very beneficial to the performance and the manageability of high performance computing (HPC) applications. However, very few HPC applications are currently running in a virtualized environment due to the performance overhead of virtualization. Further, using VMs for HPC also introduces additional challenges such as management and distribution of OS images.In this paper we present a case for HPC with virtual machines by introducing a framework which addresses the performance and management overhead associated with VM-based computing. Two key ideas in our design are: Virtual Machine Monitor (VMM) bypass I/O and scalable VM image management. VMM-bypass I/O achieves high communication performance for VMs by exploiting the OS-bypass feature of modern high speed interconnects such as Infini-Band. Scalable VM image management significantly reduces the overhead of distributing and managing VMs in large scale clusters. Our current implementation is based on the Xen VM environment and InfiniBand. However, many of our ideas are readily applicable to other VM environments and high speed interconnects.We carry out detailed analysis on the performance and management overhead of our VM-based HPC framework. Our evaluation shows that HPC applications can achieve almost the same performance as those running in a native, non-virtualized environment. Therefore, our approach holds promise to bring the benefits of VMs to HPC applications with very little degradation in performance.},
	author = {Huang, Wei and Liu, Jiuxing and Abali, Bulent and Panda, Dhabaleswar K},
	doi = {10.1145/1183401.1183421},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HUANG, WEI (2006) A Case for High Performance Computing with Virtual Machines.pdf:pdf},
	isbn = {1595932828},
	journal = {Proceedings of the 20th annual international conference on supercomputing ICS 06},
	pages = {125--134},
	title = {{A Case for High Performance Computing with Virtual Machines}},
	year = {2006}
}

@article{Youseff2008,
	abstract = {Progress of research efforts in a novel technology is contingent on having a rigorous organization of its knowledge domain and a comprehensive understanding of all the relevant components of this technology and their relationships. Cloud computing is one contemporary technology in which the research community has recently embarked. Manifesting itself as the descendant of several other computing research areas such as service-oriented architecture, distributed and grid computing, and virtualization, cloud computing inherits their advancements and limitations. Towards the end-goal of a thorough comprehension of the field of cloud computing, and a more rapid adoption from the scientific community, we propose in this paper an ontology of this area which demonstrates a dissection of the cloud into five main layers, and illustrates their interrelations as well as their inter-dependency on preceding technologies. The contribution of this paper lies in being one of the first attempts to establish a detailed ontology of the cloud. Better comprehension of the technology would enable the community to design more efficient portals and gateways for the cloud, and facilitate the adoption of this novel computing approach in scientific environments. In turn, this will assist the scientific community to expedite its contributions and insights into this evolving computing field.},
	author = {Youseff, Lamia and Butrico, Maria and {Da Silva}, Dilma},
	doi = {10.1109/GCE.2008.4738443},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/DA SILVA (2008) Toward a Unified Ontology of Cloud Computing.pdf:pdf},
	isbn = {9781424428601},
	issn = {15347362},
	journal = {Grid Computing Environments Workshop, GCE 2008},
	pmid = {21047737},
	title = {{Toward a unified ontology of cloud computing}},
	year = {2008}
}
@article{Liu2012,
	abstract = {Cloud computing is attracting increasing attention as a means of providing users with fast provisioning of computational and storage resources, elastic scaling, and payas-you-go pricing. The integration of scientific workflows and Cloud computing has the potential to significantly improve resource utilization, processing speed, and user experience. This paper proposes a novel approach for deploying bioinformatics workflows in Cloud environments using Galaxy, a platform for scientific workflows, and Globus Provision, a tool for deploying distributed computing clusters on Amazon EC2. Collectively this combination of tools provides an easy to use, high performance and scalable workflow environment that addresses the needs of data-intensive applications through dynamic cluster configuration, automatic user-defined node provisioning, high speed data transfer, and automated deployment and configuration of domain-specific software. To demonstrate how this approach can be used in practice we present a domain-specific workflow use case and performance evaluation.},
	author = {Liu, Bo and Sotomayor, Borja and Madduri, Ravi and Chard, Kyle and Foster, Ian},
	doi = {10.1109/SC.Companion.2012.131},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/Sotomayor (2012) Deploying Bioinformatics Workflows on Clouds with Galaxy and Globus Provision.pdf:pdf},
	isbn = {9780769549569},
	journal = {Proceedings - 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, SCC 2012},
	keywords = {Cloud computing,Galaxy,Globus provision,Scientific workflow},
	pages = {1087--1095},
	title = {{Deploying Bioinformatics Workflows on clouds with galaxy and globus provision}},
	year = {2012}
}
@article{Sotomayor2009,
	abstract = {Using virtual machines as a resource provisioning mechanism offers multiple benefits, most recently exploited by "infrastructure-as-a-service" clouds, but also poses several scheduling challenges. More specifically, although we can use the suspend/resume/migrate capability of virtual machines to support advance reservation of resources efficiently, by using suspension/resumption as a preemption mechanism, this requires adequately modeling the time and resources consumed by these operations to ensure that preemptions are completed before the start of a reservation. In this work we present a model for predicting various runtime overheads involved in using virtual machines, allowing us to efficiently support advance reservations. We extend our lease management software, Haizea, to use this new model in its scheduling decisions, and we use Haizea with the OpenNebula virtual infrastructure manager so the scheduling decisions will be enacted in a Xen cluster. We present both physical and simulated experimental results showing the degree of accuracy of our model and the long-term effects of variables in our model on several workloads.},
	author = {Sotomayor, Borja and Montero, Rub\'{e}n Santiago and Llorente, Ignacio Mart\'{\i}n and Foster, Ian},
	doi = {10.1109/HPCC.2009.17},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/Sotomayor (2009) Resource leasing and the art of suspending vms.pdf:pdf},
	isbn = {9780769537382},
	journal = {2009 11th IEEE International Conference on High Performance Computing and Communications, HPCC 2009},
	pages = {59--68},
	title = {{Resource leasing and the art of suspending virtual machines}},
	year = {2009}
}
@article{Sotomayor2009b,
	author={Sotomayor, B. and Montero, Ruben S. and Llorente, I.M. and Foster, I.},
	journal={Internet Computing, IEEE},
	title={Virtual Infrastructure Management in Private and Hybrid Clouds},
	year={2009},
	month={Sept},
	volume={13},
	number={5},
	pages={14-22},
	keywords={computer centres;public domain software;scheduling;virtual machines;Haizea;OpenNebula;data center management software;hybrid clouds;infrastructure-as-a-service system;open source virtual infrastructure manager;private clouds;resource lease manager;scheduling;virtual infrastructure management;virtual machines;Clouds;Open source software;Resource management;Resource virtualization;Scheduling;Virtual machining;cloud computing;distributed systems;virtual machines},
	doi={10.1109/MIC.2009.119},
	ISSN={1089-7801}}
@article{Liu2014,
	abstract = {Due to the upcoming data deluge of genome data, the need for storing and processing large-scale genome data, easy access to biomedical analyses tools, efficient data sharing and retrieval has presented significant challenges. The variability in data volume results in variable computing and storage requirements, therefore biomedical researchers are pursuing more reliable, dynamic and convenient methods for conducting sequencing analyses. This paper proposes a Cloud-based bioinformatics workflow platform for large-scale next-generation sequencing analyses, which enables reliable and highly scalable execution of sequencing analyses workflows in a fully automated manner. Our platform extends the existing Galaxy workflow system by adding data management capabilities for transferring large quantities of data efficiently and reliably (via Globus Transfer), domain-specific analyses tools preconfigured for immediate use by researchers (via user-specific tools integration), automatic deployment on Cloud for on-demand resource allocation and pay-as-you-go pricing (via Globus Provision), a Cloud provisioning tool for auto-scaling (via HTCondor scheduler), and the support for validating the correctness of workflows (via semantic verification tools). Two bioinformatics workflow use cases as well as performance evaluation are presented to validate the feasibility of the proposed approach. Ã‚Â© 2014 Elsevier Inc.},
	author = {Liu, Bo and Madduri, Ravi K. and Sotomayor, Borja and Chard, Kyle and Lacinski, Lukasz and Dave, Utpal J. and Li, Jianqiang and Liu, Chunchen and Foster, Ian T.},
	doi = {10.1016/j.jbi.2014.01.005},
	file = {:Users/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/GridComputing/LIU, SOTOMAYOR (2014) Cloud-Based\_Bioinformatics\_Workflow\_Platform\_for\_Large-Scale\_Next-Generation\_Sequencing\_Analysis.pdf:pdf},
	isbn = {1532-0480 (Electronic)
	1532-0464 (Linking)},
	issn = {15320464},
	journal = {Journal of Biomedical Informatics},
	keywords = {Bioinformatics,Cloud computing,Galaxy,Scientific workflow,Sequencing analyses},
	pages = {119--133},
	pmid = {24462600},
	publisher = {Elsevier Inc.},
	title = {{Cloud-based bioinformatics workflow platform for large-scale next-generation sequencing analyses}},
	url = {http://dx.doi.org/10.1016/j.jbi.2014.01.005},
	volume = {49},
	year = {2014}
}


@article{Burke2013,
	abstract = {Hyper-heuristics comprise a set of approaches that are motivated (at least in part) by the goal of automating the design of heuristic methods to solve hard computational search problems. An underlying strategic research challenge is to develop more generally applicable search methodologies. The term hyper-heuristic is relatively new; it was first used in 2000 to describe heuristics to choose heuristics in the context of combinatorial optimisation. However, the idea of automating the design of heuristics is not new; it can be traced back to the 1960s. The definition of hyper-heuristics has been recently extended to refer to a search method or learning mechanism for selecting or generating heuristics to solve computational search problems. Two main hyper-heuristic categories can be considered: heuristic selection and heuristic generation.The distinguishing feature of hyper-heuristics is that they operate on a search space of heuristics (or heuristic components) rather than directly on the search space of solutions to the underlying problem that is being addressed. This paper presents a critical discussion of the scientific literature on hyper-heuristics including their origin and intellectual roots, a detailed account of the main types of approaches, and an overview of some related areas. Current research trends and directions for future research are also discussed.},
	author = {Burke, Edmund K and Gendreau, Michel and Hyde, Matthew and Kendall, Graham and Ochoa, Gabriela and \"{O}zcan, Ender and Qu, Rong},
	doi = {10.1057/jors.2013.71},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HyperHeuristics/BURKE (2013) Hyper-heuristics - a survey of the state of the art.pdf:pdf},
	isbn = {0160-5682},
	issn = {0160-5682},
	journal = {Journal of the Operational Research Society},
	keywords = {combinatorial,evolutionary computation,hyper-heuristics,machine learning,metaheuristics,optimisation,scheduling},
	number = {12},
	pages = {1695--1724},
	title = {{Hyper-heuristics: a survey of the state of the art}},
	url = {http://www.palgrave-journals.com/doifinder/10.1057/jors.2013.71},
	volume = {64},
	year = {2013}
}
@article{Burke2010,
	abstract = {The current state of the art in hyper-heuristic research comprises a set of approaches that share the common goal of automating the design and adaptation of heuristic methods to solve hard computational search problems. The main goal is to produce more generally applicable search methodologies. In this chapter we present and overview of previous categorisations of hyper-heuristics and provide a unified classification and definition which captures the work that is being undertaken in this field. We distinguish between two main hyper-heuristic categories: heuristic selection and heuristic generation. Some representative examples of each category are discussed in detail. Our goal is to both clarify the main features of existing techniques and to suggest new directions for hyper-heuristic research.},
	author = {Burke, Edmund K and Hyde, Matthew and Kendall, Graham and Ochoa, Gabriela and Ozcan, Ender and Woodward, John R},
	doi = {doi:10.1007/978-1-4419-1665-5\_15},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HyperHeuristics/BURKE () A Classification of Hyper-heuristic Approaches.pdf:pdf},
	isbn = {978-1-4419-1663-1},
	issn = {0884-8289},
	journal = {Handbook of Metaheuristics},
	keywords = {genetic algorithms,genetic programming},
	pages = {449--468},
	title = {{A Classification of Hyper-heuristics Approaches}},
	volume = {57},
	year = {2010}
}
@article{Burke2009,
	abstract = {Hyper-heuristics comprise a set of approaches with the common goal of automating the design and tuning of heuristic methods to solve hard computational search problems. Themain goal is to produce more generally applicable search method- ologies. The term hyper-heuristic was coined in the early 2000s to refer to the idea of heuristics to choose heuristics. However, the idea of automating the design of com- bined heuristics can be traced back to the early 1960s. With the incorporation of Genetic Programming into hyper-heuristic research, a new type of hyper-heuristics has emerged that we have termed heuristics to generate heuristics. The distinguishing fea- ture of hyper-heuristics is that they operate on a search space of heuristics (or heuristic components) rather than directly on the search space of solutions to the underlying problem, as is the case with most meta-heuristic approaches. This paper presents a literature survey of hyper-heuristics including their origin and intellectual roots, a de- tailed account of the main types of approaches, and an overview of some related areas. Current research trends and directions for future research are also discussed.},
	author = {Burke, E K and Hyde, Matthew and Kendall, Graham and Ochoa, Gabriela and Ozcan, Ender and Qu, R},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HyperHeuristics/BURKE (2009) A Survey of Hyper-heuristics.pdf:pdf},
	journal = {Computer Science Technical Report No NOTTCS-TR-SUB-0906241418-2747, School of Computer Science and Information Technology University of Nottingham},
	title = {{A survey of hyper-heuristics}},
	url = {http://www.cs.nott.ac.uk/TR/SUB/SUB-0906241418-2747.pdf},
	year = {2009}
}
@article{Neustifter2008,
	author = {Neustifter, Andreas and Algorithmik, Seminar},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HyperHeuristics/hyper\_heuristics\_tuwien.pdf:pdf},
	title = {{Hyper-Heuristics Outline Motivation Hyper-Heuristics Using Hyper-Heuristics for 2D Bin Packing}},
	year = {2008}
}
@article{Ross2005,
	abstract = {The term â€œhyper-heuristicsï¿½ï¿½? is fairly new, although the notion has been hinted at in papers from time to time since the 1960s (e.g. Crowston et al., 1963 ). The key idea is to devise new algorithms for solving problems by combining known heuristics in ways that allow each to compensate, to some extent, for the weaknesses of others. They might be thought of as heuristics to choose heuristics . They are methods which work with a search space of heuristics. In this sense, they differ from most applications of metaheuristics (see Glover and Kochenberger, 2003 ) which usually work with search spaces of solutions. One of the main goals of research in this area is to devise algorithms that are fast and exhibit good performance across a whole family of problems, presumably because the algorithms address some shared features of the whole set of problems.},
	author = {Ross, Peter},
	doi = {10.1007/0-387-28356-0\_17},
	file = {:Users/bernd/Dropbox/arbeit/studies/OGMA/Materials\_Papers/HyperHeuristics/ross\_edinburgh\_hyper.pdf:pdf},
	isbn = {978-0-387-28356-2},
	journal = {Search Methodologies},
	pages = {529--556},
	title = {{Hyper-Heuristics}},
	url = {http://dx.doi.org/10.1007/0-387-28356-0\_17},
	year = {2005}
}
@article{King2011,
	abstract = {The reuse of scientific knowledge obtained from one investigation in another investigation is basic to the advance of science. Scientific investigations should therefore be recorded in ways that promote the reuse of the knowledge they generate. The use of logical formalisms to describe scientific knowledge has potential advantages in facilitating such reuse. Here, we propose a formal framework for using logical formalisms to promote reuse. We demonstrate the utility of this framework by using it in a worked example from biology: demonstrating cycles of investigation formalization [F] and reuse [R] to generate new knowledge. We first used logic to formally describe a Robot scientist investigation into yeast (Saccharomyces cerevisiae) functional genomics [f(1)]. With Robot scientists, unlike human scientists, the production of comprehensive metadata about their investigations is a natural by-product of the way they work. We then demonstrated how this formalism enabled the reuse of the research in investigating yeast phenotypes [r(1) = R(f(1))]. This investigation found that the removal of non-essential enzymes generally resulted in enhanced growth. The phenotype investigation was then formally described using the same logical formalism as the functional genomics investigation [f(2) = F(r(1))]. We then demonstrated how this formalism enabled the reuse of the phenotype investigation to investigate yeast systems-biology modelling [r(2) = R(f(2))]. This investigation found that yeast flux-balance analysis models fail to predict the observed changes in growth. Finally, the systems biology investigation was formalized for reuse in future investigations [f(3) = F(r(2))]. These cycles of reuse are a model for the general reuse of scientific knowledge.},
	author = {King, Ross D and Liakata, Maria and Lu, Chuan and Oliver, Stephen G and Soldatova, Larisa N},
	doi = {10.1098/rsif.2011.0029},
	file = {:home/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/MetaLearning/KING (2011) On the formalization and reuse of scientific research.pdf:pdf},
	isbn = {1742-5662 (Electronic)$\backslash$r1742-5662 (Linking)},
	issn = {1742-5689},
	journal = {Journal of the Royal Society, Interface / the Royal Society},
	keywords = {logic,ontology,saccharomyces cerevisiae,semantic web},
	number = {63},
	pages = {1440--1448},
	pmid = {21490004},
	title = {{On the formalization and reuse of scientific research.}},
	volume = {8},
	year = {2011}
}
@article{SmithMiles2008,
	abstract = {In this paper we propose a meta-learning inspired framework for analysing the performance of meta-heuristics for optimization problems, and developing insights into the relationships between search space characteristics of the problem instances and algorithm performance. Preliminary results based on several meta-heuristics for well-known instances of the Quadratic Assignment Problem are presented to illustrate the approach using both supervised and unsupervised learning methods.},
	author = {Smith-Miles, Kate a.},
	doi = {10.1109/IJCNN.2008.4634391},
	file = {:home/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/AlgorithmSelection/MILES (2008) Towards Insightful Algorithm Selection For Optimisaztion Using Meta-Learning Concepts.pdf:pdf},
	isbn = {9781424418213},
	issn = {1098-7576},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	pages = {4118--4124},
	title = {{Towards insightful algorithm selection for optimisation using meta-learning concepts}},
	year = {2008}
}
@article{Rice1975,
	author = {Rice, John R},
	file = {:home/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/HyperHeuristics/RICE (1976) The Algorithm Selection Problem.pdf:pdf},
	journal = {Advances in Computers},
	pages = {65--117},
	title = {{The algorithm selection problem}},
	volume = {15},
	year = {1975}
}
@article{Hutter2007,
	author = {Hutter, Frank and Hutter, Frank and Hoos, Holger H and Hoos, Holger H},
	file = {:home/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/HyperHeuristics/HUTTER (2007) Automatic Algorithm Configuration based on Local Search.pdf:pdf},
	journal = {BMC Bioinformatics},
	pages = {1152--1157},
	title = {{Automatic Algorithm Configuration based on Local Search}},
	year = {2007}
}
@article{Horvitz2001,
	abstract = {We describe research and results centering on the construction and use of Bayesian models that can predict the run time of problem solvers. Our efforts are motivated by observations of high variance in the time required to solve instances for several challenging problems. The methods have application to the decision-theoretic control of hard search and reasoning algorithms. We illustrate the approach with a focus on the task of predicting run time for general and domain-specific solvers on a hard class of structured constraint satisfaction problems. We review the use of learned models to predict the ultimate length of a trial, based on observing the behavior of the search algorithm during an early phase of a problem session. Finally, we discuss how we can employ the models to inform dynamic run-time decisions.},
	author = {Horvitz, Eric and Ruan, Yongshao and Gomes, Carla and Kautz, Henry and Selman, Bart and Chickering, Max},
	doi = {10.1016/S1571-0653(04)00335-X},
	file = {:home/bernd/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/HyperHeuristics/HORVITZ et al (2001) A Bayesian Approach to Tackling Hard Computational Problems.pdf:pdf},
	isbn = {1-55860-800-1},
	issn = {15710653},
	journal = {Electronic Notes in Discrete Mathematics},
	pages = {376--391},
	title = {{A Bayesian Approach to Tackling Hard Computational Problems (Preliminary Report)}},
	volume = {9},
	year = {2001}
}
@article{Holzinger2013,
	abstract = {BACKGROUND: Professionals in the biomedical domain are confronted with an increasing mass of data. Developing methods to assist professional end users in the field of Knowledge Discovery to identify, extract, visualize and understand useful information from these huge amounts of data is a huge challenge. However, there are so many diverse methods and methodologies available, that for biomedical researchers who are inexperienced in the use of even relatively popular knowledge discovery methods, it can be very difficult to select the most appropriate method for their particular research problem. RESULTS: A web application, called KNODWAT (KNOwledge Discovery With Advanced Techniques) has been developed, using Java on Spring framework 3.1. and following a user-centered approach. The software runs on Java 1.6 and above and requires a web server such as Apache Tomcat and a database server such as the MySQL Server. For frontend functionality and styling, Twitter Bootstrap was used as well as jQuery for interactive user interface operations. CONCLUSIONS: The framework presented is user-centric, highly extensible and flexible. Since it enables methods for testing using existing data to assess suitability and performance, it is especially suitable for inexperienced biomedical researchers, new to the field of knowledge discovery and data mining. For testing purposes two algorithms, CART and C4.5 were implemented using the WEKA data mining framework.},
	author = {Holzinger, Andreas and Zupan, Mario},
	doi = {10.1186/1471-2105-14-191},
	file = {:C$\backslash$:/Users/Bernd Malle/Dropbox/arbeit/Studies/OGMA/Materials\_Papers/holzinger\_zupan.pdf:pdf},
	isbn = {1471-2105 (Linking)},
	issn = {1471-2105},
	journal = {BMC Bioinformatics},
	keywords = {data analytics,knowledge discovery,methods},
	pages = {191},
	pmid = {23763826},
	title = {{KNODWAT : A scientific framework application for testing knowledge discovery methods for the biomedical domain}},
	url = {http://www.biomedcentral.com/1471-2105/14/191/abstract},
	volume = {14},
	year = {2013}
}
@article{Pandit2007,
	abstract = {Given a large online network of online auction users and their histories of transactions, how can we spot anomalies and auction fraud? This paper describes the design and implementation of NetProbe, a system that we propose for solving this problem. NetProbe models auction users and transactions as a Markov Random Field tuned to detect the suspicious patterns that fraudsters create, and employs a Belief Propagation mechanism to detect likely fraudsters. Our experiments show that NetProbe is both efficient and effective for fraud detection. We report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges, where NetProbe was able to spot fraudulent nodes with over 90\% precision and recall, within a matter of seconds. We also report experiments on a real dataset crawled from eBay, with nearly 700,000 transactions between more than 66,000users, where NetProbe was highly effective at unearthing hidden networks of fraudsters, within a realistic response time of about 6 minutes. For scenarios where the underlying data is dynamic in nature, we propose IncrementalNetProbe, which is an approximate, but fast, variant of NetProbe. Our experiments prove that Incremental NetProbe executes nearly doubly fast as compared to NetProbe, while retaining over 99\% of its accuracy.},
	author = {Pandit, Shashank and Chau, DH and Wang, Samuel and Faloutsos, Christos},
	doi = {10.1145/1242572.1242600},
	isbn = {9781595936547},
	journal = {Proceedings of the 16th \ldots},
	pages = {210, 201},
	title = {{Netprobe: a fast and scalable system for fraud detection in online auction networks}},
	url = {http://dx.doi.org/10.1145/1242572.1242600$\backslash$nhttp://dl.acm.org/citation.cfm?id=1242600},
	volume = {42},
	year = {2007}
}
@article{Raz2002risk,
	title={Risk management, project success, and technological uncertainty},
	author={Raz, Tzvi and Shenhar, Aaron J and Dvir, Dov},
	journal={R\&D Management},
	volume={32},
	number={2},
	pages={101--109},
	year={2002},
	publisher={Wiley Online Library}
}
@article{Perminova2008,
	title={Defining uncertainty in projects--a new perspective},
	author={Perminova, Olga and Gustafsson, Magnus and Wikstr{\"o}m, Kim},
	journal={International Journal of Project Management},
	volume={26},
	number={1},
	pages={73--79},
	year={2008},
	publisher={Elsevier}
}
@book{Holzinger2010ProcessGuide,
	title={Process Guide for Students for Interdisciplinary Work in Computer Science/Informatics.
	Second Edition},
	author={Holzinger, Andreas},
	year={2010},
	address={Norderstedt},
	publisher={BoD}
}

@book{Holzinger2012biomedical,
	title={Biomedical Informatics: Computational Sciences meets Life Sciences},
	author={Holzinger, Andreas},
	year={2012},
	address={Norderstedt},
	publisher={BoD}
}

@ONLINE{AndrewNGCeiling,
	title  = "What part of the pipeline to work on next",
	author = "Andrew Ng",
	url    = "https://class.coursera.org/ml-005/lecture/113",
	Note   = "\url{https://class.coursera.org/ml-005/lecture/113}",
	year   = "2013"
}


@inproceedings{RecCascades,
	author = {Leskovec, Jure and Singh, Ajit and Kleinberg, Jon},
	title = {Patterns of Influence in a Recommendation Network},
	booktitle = {Proceedings of the 10th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining},
	series = {PAKDD'06},
	year = {2006},
	isbn = {3-540-33206-5, 978-3-540-33206-0},
	location = {Singapore},
	pages = {380--389},
	numpages = {10},
	url = {http://dx.doi.org/10.1007/11731139_44},
	doi = {10.1007/11731139_44},
	acmid = {2097097},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
}

@article{JordanMitchell:2015:MLtrendsScience,
	year = {2015},
	author = {Jordan, M. I. and Mitchell, T. M.},
	title = {Machine learning: Trends, perspectives, and prospects},
	journal = {Science},
	volume = {349},
	number = {6245},
	pages = {255-260},
	abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of todayâ€™s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
	doi = {10.1126/science.aaa8415},
	url = {http://www.sciencemag.org/content/349/6245/255.abstract}
}

@article{Foxx2014,
	author = {Dohmen, L., Edlich, I.S. and Hackstein, M.},
	title = {A Declarative Web Framework for the Server-side Extension of the Multi Model Database ArangoDB},
	year = {2014}
}

@book{hahn2013javascript,
	title={JavaScript Testing with Jasmine},
	author={Hahn, Evan},
	year={2013},
	publisher={" O'Reilly Media, Inc."}
}