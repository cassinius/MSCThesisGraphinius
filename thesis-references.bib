%references for the Holzinger group - checked 02.04.2016


@article{Holzinger:2016:iML,
   year = {2016},
   author = {Holzinger, Andreas},
   title = {Interactive Machine Learning for Health Informatics: When do we need the human-in-the-loop?},
   journal = {Springer Brain Informatics (BRIN)},
   volume = {3},
   pages = {1-13},
   abstract = {Machine learning (ML) is the fastest growing field in computer science, and health informatics is amongst the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic Machine Learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive Machine Learning (iML) may be of help, having its roots in Reinforcement Learning (RL), Preference Learning (PL) and Active Learning (AL). The term iML is not yet well used, so we define it as algorithms that can interact with agents and can optimize their learning behaviour through these interactions, where the agents can also be human. This human-in-the-loop can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.},
   keywords = {interactive Machine learning, health informatics},
   doi = {10.1007/s40708-016-0042-6},
   url = {http://dx.doi.org/10.1007/s40708-016-0042-6}
}



@article{Samuel:1959:machineLearningCheckers,
   year = {1959},
   author = {Samuel, Arthur L},
   title = {Some studies in machine learning using the game of checkers},
   journal = {IBM Journal of research and development},
   volume = {3},
   number = {3},
   pages = {210-229},
   abstract = {In the 1950s, Arthur Samuel created one of the first board game-playing programs of any kind. More recently, in 2007 scientists at the University of Alberta developed their "Chinook" program to the point where it is unbeatable. A brute force approach that took hundreds of computers working nearly two decades was used to solve the game,[18] showing that a game of draughts will always end in a draw if neither player makes a mistake.[19][20] The solution is for the draughts variation called go-as-you-please (GAYP) checkers and not for the variation called three-move restriction checkers. As of December 2007, this makes English draughts the most complex game ever solved.}
}

@article{JordanMitchell:2015:MLtrendsScience,
   year = {2015},
   author = {Jordan, M. I. and Mitchell, T. M.},
   title = {Machine learning: Trends, perspectives, and prospects},
   journal = {Science},
   volume = {349},
   number = {6245},
   pages = {255-260},
   abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
   doi = {10.1126/science.aaa8415},
   url = {http://www.sciencemag.org/content/349/6245/255.abstract}
} 

@article{LeCunBengioHinton:2015:DeepLearningNature,
   year = {2015},
   author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
   title = {Deep learning},
   journal = {Nature},
   volume = {521},
   number = {7553},
   pages = {436-444},
   abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
   doi = {10.1038/nature14539}
}


@article{1763:BayesOriginal,
   year = {1763},
   author = {Bayes, Thomas},
   title = {An Essay towards solving a Problem in the Doctrine of Chances (Posthumous communicated by Richard Price)},
   journal = {Philosophical Transactions},
   volume = {53},
   pages = {370-418}
}

@article{Barnard:1958:OnBayes,
   year = {1958},
   author = {Barnard, George A and Bayes, Thomas},
   title = {Studies in the history of probability and statistics: IX. Thomas Bayes's essay towards solving a problem in the doctrine of chances},
   journal = {Biometrika},
   volume = {45},
   number = {3/4},
   pages = {293-315},
   doi = {10.2307/2333180},
   url = {http://www.jstor.org/stable/2333180}
}

@book{HastieTibshiraniFriedman:2009:ElementsOfstatisticalLearning,
   year = {2009},
   author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
   title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second Edition},
   publisher = {Springer},
   address = {New York},
   abstract = {The field of Statistics is constantly challenged by the problems that science and industry brings to its door. In the early days, these problems often came from agricultural and industrial experiments and were relatively small in scope. With the advent of computers and the information age, statistical problems have exploded both in size and complexity. Challenges in the areas of data storage, organization and searching have led to the new field of “data mining”; statistical and computational problems in biology and medicine have created “bioinformatics.” Vast amounts of data are being generated in many fields, and the statistician’s job is to make sense of it all: to extract important patterns and trends, and understand “what the data says.” We call this learning from data. The challenges in learning from data have led to a revolution in the statistical sciences. Since computation plays such a key role, it is not surprising that much of this new development has been done by researchers in other fields such as computer science and engineering. The learning problems that we consider can be roughly categorized as either supervised or unsupervised. In supervised learning, the goal is to predict the value of an outcome measure based on a number of input measures; in unsupervised learning, there is no outcome measure, and the goal is to describe the associations and patterns among a set of input measures.},
   keywords = {Machine learning},
   doi = {10.1007/978-0-387-84858-7}
}


@book{Murphy:2012:MLbook,
   year = {2012},
   author = {Murphy, Kevin P},
   title = {Machine learning: a probabilistic perspective},
   publisher = {MIT press},
   address = {Cambridge (MA)},
   abstract = {This books adopts the view that the best way to make machines that can learn from data is to use the tools of probability theory, which has been the mainstay of statistics and engineering for centuries. Probability theory can be applied to any problem involving uncertainty. In machine learning, uncertainty comes in many forms: what is the best prediction (or decision) given some data? what is the best model given some data? what measurement should I perform next? etc. The systematic application of probabilistic reasoning to all inferential problems, including inferring parameters of statistical models, is sometimes called a Bayesian approach. However, this term tends to elicit very strong reactions (either positive or negative, depending on who you ask), so we prefer the more neutral term “probabilistic approach”. Besides, we will often use techniques such as maximum likelihood estimation, which are not Bayesian methods, but certainly fall within the probabilistic paradigm.},
   url = {http://www.cs.ubc.ca/~murphyk/MLbook/index.html}
}


@article{Holzinger:2014:trends,
   year = {2014},
   author = {Holzinger, Andreas},
   title = {Trends in Interactive Knowledge Discovery for Personalized Medicine: Cognitive Science meets Machine Learning},
   journal = {IEEE Intelligent Informatics Bulletin},
   volume = {15},
   number = {1},
   pages = {6-14},
   abstract = {A grand goal of future medicine is in modelling the complexity of patients to tailor medical decisions, health practices and therapies to the individual patient. This trend towards personalized medicine produces unprecedented amounts of data, and even though the fact that human experts are excellent at pattern recognition in dimensions of smaller than three, the problem is that most biomedical data is in dimensions much higher than three, making manual analysis difficult and often impossible. Experts in daily medical routine are decreasingly capable of dealing with the complexity of such data. Moreover, they are not interested the data, they need knowledge and insight in order to support their work. Consequently, a big trend in computer science is to provide efficient, useable and useful computational methods, algorithms and tools to discover knowledge and to interactively gain insight into high-dimensional data. A synergistic combination of methodologies of two areas may be of great help here: Human–Computer Interaction (HCI) and Knowledge Discovery/Data Mining (KDD), with the goal of supporting human intelligence with machine learning. A trend in both disciplines is the acquisition and adaptation of representations that support efficient learning. Mapping higher dimensional data into lower dimensions is a major task in HCI, and a concerted effort of computational methods including recent advances from graphtheory and algebraic topology may contribute to finding solutions. Moreover, much biomedical data is sparse, noisy and timedependent, hence entropy is also amongst promising topics. This paper provides a rough overview of the HCI-KDD approach and focuses on three future trends: graph-based mining, topological data mining and entropy-based data mining.[interactive machine learning]},
url = {http://www.comp.hkbu.edu.hk/~cib/2014/Dec/article2/iib_vol15no1_article2.pdf}
}

@book{Mitchell:1997:MachineLearningBook,
   year = {1997},
   author = {Mitchell, Tom M},
   title = {Machine learning},
   publisher = {McGraw Hill},
   address = {New York},
   abstract = {The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience. In recent  years many successful machine learning applications have been developed, ranging from data-mining programs that learn to detect fraudulent credit card transactions, to information-filtering systems that learn users'  reading preferences, to autonomous vehicles that learn to drive on public highways. At the same time, there have been important advances in the theory and algorithms that form the foundations of this field.}
}

@article{PetzEtAl:2015:Sentiment,
   year = {2015},
   author = {Petz, Gerald and Karpowicz, Michał and Fürschuß, Harald and Auinger, Andreas and Stříteský, Václav and Holzinger, Andreas},
   title = {Computational approaches for mining user’s opinions on the Web 2.0},
   journal = {Information Processing \& Management},
   volume = {51},
   number = {4},
   pages = {510-519},
   abstract = {The emerging research area of opinion mining deals with computational methods in order to find, extract and systematically analyze people’s opinions, attitudes and emotions towards certain topics. While providing interesting market research information, the user generated content existing on the Web 2.0 presents numerous challenges regarding systematic analysis, the differences and unique characteristics of the various social media channels being one of them. This article reports on the determination of such particularities, and deduces their impact on text preprocessing and opinion mining algorithms. The effectiveness of different algorithms is evaluated in order to determine their applicability to the various social media channels. Our research shows that text preprocessing algorithms are mandatory for mining opinions on the Web 2.0 and that part of these algorithms are sensitive to errors and mistakes contained in the user generated content.},
   keywords = {Opinion mining},
   doi = {http://dx.doi.org/10.1016/j.ipm.2014.07.011},
   url = {http://www.sciencedirect.com/science/article/pii/S0306457315000655}
}

@article{HolzingerEtAl:2014:KDDBio,
   year = {2014},
   author = {Holzinger, Andreas and Dehmer, Matthias and Jurisica, Igor},
   title = {Knowledge Discovery and interactive Data Mining in Bioinformatics - State-of-the-Art, future challenges and research directions},
   journal = {BMC Bioinformatics},
   volume = {15},
   number = {S6},
   pages = {I1},
   abstract = {The life sciences, biomedicine and health care are increasingly turning into a data intensive science. Particularly in bioinformatics and computational biology we face not only increased volume and a diversity of highly complex, multi-dimensional and often weakly-structured and noisy data, but also the growing need for integrative analysis and modeling. Due to the increasing trend towards personalized and precision medicine (P4 medicine: Predictive, Preventive, Participatory, Personalized), biomedical data today results from various sources in different structural dimensions, ranging from the microscopic world, and in particular from the omics world (e.g., from genomics, proteomics, metabolomics, lipidomics, transcriptomics, epigenetics, microbiomics, fluxomics, phenomics, etc.) to the macroscopic world (e.g., disease spreading data of populations in public health informatics). The challenge is not only to extract meaningful information from this data, but to gain knowledge, to discover previously unknown insight, look for patterns, and to make sense of the data. },
   keywords = {Knowledge Discovery, Interactive Data Mining, Bioinformatics, Biomedical Informatics, Data intensive Science},
   doi = {doi:10.1186/1471-2105-15-S6-I1},
   url = {http://www.biomedcentral.com/1471-2105/15/S6/I1}
}

@book{Holzinger:2014:SpringerTextbook,
   year = {2014},
   author = {Holzinger, Andreas},
   title = {Biomedical Informatics: Discovering Knowledge in Big Data},
   publisher = {Springer},
   address = {New York},
   abstract = {This book provides a broad overview of the topic Bioinformatics (medical informatics + biological information) with a focus on data, information and knowledge. From data acquisition and storage to visualization, privacy, regulatory, and other practical and theoretical topics, the author touches on several fundamental aspects of the innovative interface between the medical and computational domains that form biomedical informatics. Each chapter starts by providing a useful inventory of definitions and commonly used acronyms for each topic, and throughout the text, the reader finds several real-world examples, methodologies, and ideas that complement the technical and theoretical background. Also at the beginning of each chapter a new section called key problems, has been added, where the author discusses possible traps and unsolvable or major problems. This new edition includes new sections at the end of each chapter, called future outlook and research avenues, providing pointers to future challenges.},
   doi = {10.1007/978-3-319-04528-3}
}

@incollection{Holzinger:2013:HCI-KDD,
   year = {2013},
   author = {Holzinger, Andreas},
   title = {Human-–Computer Interaction and Knowledge Discovery ({HCI-KDD}): What is the benefit of bringing those two fields to work together?},
   booktitle = {Multidisciplinary Research and Practice for Information Systems, Springer Lecture Notes in Computer Science LNCS 8127},
   editor = {Cuzzocrea, Alfredo and Kittl, Christian and Simos, Dimitris E. and Weippl, Edgar and Xu, Lida},
   publisher = {Springer},
   address = {Heidelberg, Berlin, New York},
   pages = {319-328},
   abstract = {A major challenge in our networked world is the increasing amount of data, which require efficient and user-friendly solutions. A timely example is the biomedical domain: the trend towards personalized medicine has resulted in a sheer mass of the generated (-omics) data. In the life sciences domain, most data models are characterized by complexity, which makes manual analysis very time-consuming and frequently practically impossible. Computational methods may help; however, we must acknowledge that the problem-solving knowledge is located in the human mind and – not in machines. A strategic aim to find solutions for data intensive problems could lay in the combination of two areas, which bring ideal pre-conditions: Human–Computer Interaction (HCI) and Knowledge Discovery (KDD). HCI deals with questions of human perception, cognition, intelligence, decision-making and interactive techniques of visualization, so it centers mainly on supervised methods. KDD deals mainly with questions of machine intelligence and data mining, in particular with the development of scalable algorithms for finding previously unknown relationships in data, thus centers on automatic computational methods. A proverb attributed perhaps incorrectly to Albert Einstein illustrates this perfectly: “Computers are incredibly fast, accurate, but stupid. Humans are incredibly slow, inaccurate, but brilliant. Together they may be powerful beyond imagination”. Consequently, a novel approach is to combine HCI & KDD in order to enhance human intelligence by computational intelligence. },
   keywords = {Human-Computer Interaction (HCI), Knowledge Discovery in Data (KDD), HCI-KDD, E-Science, Interdisciplinary, Intersection science},
   url = {https://online.tugraz.at/tug_online/voe_main2.getVollText?pDocumentNr=382991&pCurrPk=72064}
}



